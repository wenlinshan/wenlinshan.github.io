<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>ClickHouse 中其它常见的表引擎(八) | 热心市民温温</title><meta name="author" content="wenwen"><meta name="copyright" content="wenwen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ClickHouse 中其它常见的表引擎(八)​																	本文来源： ( https:&#x2F;&#x2F;www.cnblogs.com&#x2F;traditional&#x2F;tag&#x2F;ClickHouse：一款速度快到让人发指的列式存储数据库&#x2F; )   楔子Everything is table（万物皆为表）是 ClickHouse 的一个非常有意思的设计思路，正因为 ClickHouse 是一款数据库">
<meta property="og:type" content="article">
<meta property="og:title" content="ClickHouse 中其它常见的表引擎(八)">
<meta property="og:url" content="https://hesay.cn/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/index.html">
<meta property="og:site_name" content="热心市民温温">
<meta property="og:description" content="ClickHouse 中其它常见的表引擎(八)​																	本文来源： ( https:&#x2F;&#x2F;www.cnblogs.com&#x2F;traditional&#x2F;tag&#x2F;ClickHouse：一款速度快到让人发指的列式存储数据库&#x2F; )   楔子Everything is table（万物皆为表）是 ClickHouse 的一个非常有意思的设计思路，正因为 ClickHouse 是一款数据库">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hesay.cn/img/favicon.png">
<meta property="article:published_time" content="2023-04-11T09:04:31.000Z">
<meta property="article:modified_time" content="2023-04-13T09:06:57.974Z">
<meta property="article:author" content="wenwen">
<meta property="article:tag" content="ClickHouse">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hesay.cn/img/favicon.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://hesay.cn/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ClickHouse 中其它常见的表引擎(八)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-04-13 17:06:57'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/favicon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="热心市民温温"><span class="site-name">热心市民温温</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">ClickHouse 中其它常见的表引擎(八)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-04-11T09:04:31.000Z" title="发表于 2023-04-11 17:04:31">2023-04-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-04-13T09:06:57.974Z" title="更新于 2023-04-13 17:06:57">2023-04-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ClickHouse/">ClickHouse</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ClickHouse 中其它常见的表引擎(八)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="ClickHouse-中其它常见的表引擎-八"><a href="#ClickHouse-中其它常见的表引擎-八" class="headerlink" title="ClickHouse 中其它常见的表引擎(八)"></a>ClickHouse 中其它常见的表引擎(八)</h1><p>​																	本文来源： ( <a target="_blank" rel="noopener" href="https://www.cnblogs.com/traditional/tag/ClickHouse%EF%BC%9A%E4%B8%80%E6%AC%BE%E9%80%9F%E5%BA%A6%E5%BF%AB%E5%88%B0%E8%AE%A9%E4%BA%BA%E5%8F%91%E6%8C%87%E7%9A%84%E5%88%97%E5%BC%8F%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E5%BA%93/">https://www.cnblogs.com/traditional/tag/ClickHouse：一款速度快到让人发指的列式存储数据库/</a> ) </p>
<hr>
<h3 id="楔子"><a href="#楔子" class="headerlink" title="楔子"></a>楔子</h3><p><strong>Everything is table（万物皆为表）是 ClickHouse 的一个非常有意思的设计思路，正因为 ClickHouse 是一款数据库，所以自然而然数据表就是它的武器，是它与外部进行交互的接口层。在数据表背后无论连接的是本地文件、HDFS、zookeeper，还是其它服务，终端用户只需要面对数据表，只需要使用 SQL 查询语言。</strong></p>
<p><strong>下面就来介绍一下其它类型的表引擎，它们以表为接口，极大地丰富了 ClickHouse 的查询能力。这些表引擎各自特点突出，或是独立地应用于特定场景，或是能够与 MergeTree 搭配使用。例如外部存储系列的表引擎，能够直接读取其它系统的数据，ClickHouse 自身只负责元数据的管理，类似使用外部表的形式；内存系列的表引擎，能够充当数据分发的临时存储载体或消息通道；日志文件系列的表引擎，拥有简单易用的特点；接口系列的表引擎，能够串联已有数据表，起到粘合剂的作用。那么下面我们就来分门别类的介绍一下，这些表引擎各自的使用特点。</strong></p>
<h3 id="外部存储类型"><a href="#外部存储类型" class="headerlink" title="外部存储类型"></a>外部存储类型</h3><p><strong>顾名思义，外部存储表引擎能够直接从其它的存储系统读取数据，例如直接读取 HDFS 的文件或者 MySQL 数据库的表，这些表引擎只负责元数据管理和数据查询，而它们自身通常不负责数据的写入，数据文件直接由外部系统提供。</strong></p>
<h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><p><strong>HDFS 是一款分布式文件存储系统，可以说是 Hadoop 生态的基石，而 ClickHouse 提供的 HDFS 表引擎则可以与之对接，读取 HDFS 内的文件。关于 HDFS 的安装这里不赘述了，这里假设已经安装完毕。但是注意，我们需要关闭 HDFS 的 Kerberos 认证，因为 HDFS 表引擎还不支持 Kerberos，然后在 HDFS 上创建用于存放文件的目录。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /clickhouse</span><br></pre></td></tr></table></figure>

<p><strong>最后在 HDFS 上给 clickhouse 用户授权：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -chown -R clickhouse:clickhouse /clickhouse</span><br></pre></td></tr></table></figure>

<p><strong>然后我们创建 HDFS 数据表，而 ClickHouse 的一张 HDFS 数据表，对应 HDFS 文件系统上的一个文件：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hdfs_table1 (</span><br><span class="line">    id UInt32,</span><br><span class="line">    code String,</span><br><span class="line">    name String</span><br><span class="line">) ENGINE <span class="operator">=</span> HDFS(<span class="string">&#x27;hdfs://localhost:6666/clickhouse/hdfs_table1&#x27;</span>, <span class="string">&#x27;CSV&#x27;</span>)</span><br><span class="line"><span class="comment">-- HDFS(&#x27;HDFS 的文件存储路径&#x27;, &#x27;文件格式，如 CSV、TSV、JSON 等等&#x27;)</span></span><br><span class="line"><span class="comment">-- 注：数据表的名字和 HDFS 文件系统上的文件名可以不一致</span></span><br></pre></td></tr></table></figure>

<p><strong>注意：我们这里虽然创建了一张表 hdfs_table1，但 HDFS 文件系统上还并没有 hdfs_table1 这个文件，而当我们往表中插入数据时，表对应的文件就会在 HDFS 文件系统上创建、同时将数据写进去。因此我们写入数据虽然表面上是通过 HDFS 数据表，但实际上数据是存储在 HDFS 文件系统上的，而 ClickHouse 在这里只负责元数据的管理。可能有人发现了，这不就是 Hive 嘛，是的，ClickHouse 在这里所干的事情和 Hive 是一样的。下面写入一批数据：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> hdfs_table1 </span><br><span class="line"><span class="keyword">SELECT</span> number, concat(<span class="string">&#x27;code&#x27;</span>, toString(number)), concat(<span class="string">&#x27;n&#x27;</span>, toString(number))</span><br><span class="line"><span class="keyword">FROM</span> numbers(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p><strong>此时在 HDFS 文件系统的 &#x2F;clickhouse 下面会创建一个文件，也叫 hdfs_table1，同时将数据写进去。然后我们就可以通过数据表查询，注意：因为数据存在 HDFS 文件系统上，所以查询实际上就是 ClickHouse 读取 HDFS 文件系统的一个过程。</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002619076-698383201.png" alt="img"></p>
<p><strong>然后我们再来看看 HDFS 上文件：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002626203-1247824117.png" alt="img"></p>
<p><strong>可以发现通过 HDFS 表引擎，ClickHouse 在 HDFS 的指定目录下创建了一个名为 hdfs_table1 的文件，并且按照 CSV 格式写入了数据。注意：这里创建的数据表类似于 hive 中的外部表，也就是说将 HDFS 数据表删除（删除元数据），并不会影响 HDFS 文件系统上的文件。</strong></p>
<p><strong>以上就是 ClickHouse 和 HDFS 之间的交互，不过我们知道 ClickHouse 具有分片功能（后面说），所以它完全不需要借助于 HDFS 存储系统来存储数据，而且使用 HDFS 的话，那么 ClickHouse 的列式存储、数据压缩、索引等一系列高级特性就都用不上了，反而会严重拖慢 ClickHouse 的效率。但 ClickHouse 之所以还提供和 HDFS 的交互，主要是考虑到 Hadoop 生态圈已经存在多年了，在 HDFS 之上已经存储了大量的数据，所以提供了和 HDFS 交互的接口。通过 HDFS 数据表将 HDFS 文件系统上的数据读取出来之后，导入到 MergeTree 数据表中，然后进行数据分析。</strong></p>
<p><strong>所以 HDFS 数据表虽然既负责写又负责读，就像我们上面演示的那样，但很明显我们基本不会用 HDFS 数据表写数据。因此当涉及 ClickHouse 和 HDFS 的交互时，都是数据已经存在于 HDFS 文件系统之上，我们只是创建一个 HDFS 数据表将数据从 HDFS 文件系统上读取出来罢了。所以此时创建 HDFS 数据表就需要根据文件内容来创建了。</strong></p>
<p><strong>比如 HDFS 上存在一个 CSV 文件，这个文件里面有 4 列，那么我们创建的数据表就应该有 4 个字段。举个栗子：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002633883-1719103428.png" alt="img"></p>
<p><strong>此时 HDFS 上有一个 TSV 格式的文件（CSV 文件的分隔符为逗号，TSV 文件的分隔符为 \t），这个时候我们需要使用 ClickHouse 将其读取出来。具体做法显然是创建一张 HDFS 数据表，然后指定数据文件在 HDFS 上存储路径即可，但问题是表字段要如何设计呢？没错，显然要根据文件的存储内容来进行设计，比如这里有 4 个列，那么 HDFS 数据表就应该要有 4 个字段，然后再根据存储的内容指定字段的类型，那么这个 HDFS 数据表就可以这么定义：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hdfs_table2 (</span><br><span class="line">    a UInt32,</span><br><span class="line">    b String,</span><br><span class="line">    c UInt32,</span><br><span class="line">    d String</span><br><span class="line">) ENGINE <span class="operator">=</span> HDFS(<span class="string">&#x27;hdfs://localhost:6666/clickhouse/hdfs_table2&#x27;</span>, <span class="string">&#x27;TSV&#x27;</span>);</span><br><span class="line"><span class="comment">-- 文件类型要指定 TSV，因为分隔符是 \t</span></span><br><span class="line"><span class="comment">-- 注：这里字段名叫什么完全由我们自己定义，我们也可以起一个有意义的名字</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hdfs_table2_new (</span><br><span class="line">    id UInt32,</span><br><span class="line">    name String,</span><br><span class="line">    age UInt32,</span><br><span class="line">    place String</span><br><span class="line">) ENGINE <span class="operator">=</span> HDFS(<span class="string">&#x27;hdfs://localhost:6666/clickhouse/hdfs_table2&#x27;</span>, <span class="string">&#x27;TSV&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p><strong>这里我们的两张 HDFS 数据表都指向 HDFS 文件系统上的同一个文件：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002641127-2080962518.png" alt="img"></p>
<p><strong>还是比较简单的，这里的 ClickHouse 完全就充当了 Hive 的角色，甚至比 Hive 还要好用不少。不过 ClickHouse 支持的还不止这些，在指定 HDFS 文件路径的时候 ClickHouse 支持多种方式：</strong></p>
<ul>
<li><code>绝对路径：会读取指定路径的单个文件，比如HDFS(&#39;hdfs://localhost:6666/clickhouse/hdfs_table2&#39;, &#39;TSV&#39;)，会读取 clickhouse 目录下的 hdfs_table2 文件</code></li>
<li><code>* 通配符：匹配任意数量的任意字符，比如 HDFS(&#39;hdfs://localhost:6666/clickhouse/*&#39;, &#39;TSV&#39;)，会读取 clickhouse 目录下的所有文件</code></li>
<li><code>? 通配符：匹配单个任意字符，比如 ENGINE = HDFS(&#39;hdfs://localhost:6666/clickhouse/hdfs_table?&#39;, &#39;TSV&#39;)，会读取 clickhouse 目录下所有匹配 hdfs_table? 的文件</code></li>
<li><code>&#123;M..N&#125; 数字区间：匹配指定数字的文件，例如 HDFS(&#39;hdfs://localhost:6666/clickhouse/hdfs_table&#123;1..3&#125;&#39;, &#39;TSV&#39;)，会读取 clickhouse 目录下的 hdfs_table1、hdfs_table2、hdfs_table3</code></li>
</ul>
<p><strong>我们来测试一下，我们上面的文件都没有后缀名，但有后缀名也是可以的。</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002647181-598571969.png" alt="img"></p>
<p><strong>这里我们将之前的 hdfs_table2 拷贝 3 份，并上传至 HDFS，然后创建数据表：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> girls (</span><br><span class="line">    id UInt32,</span><br><span class="line">    name String,</span><br><span class="line">    age UInt32,</span><br><span class="line">    place String</span><br><span class="line">) ENGINE <span class="operator">=</span> HDFS(<span class="string">&#x27;hdfs://localhost:6666/clickhouse/girls_&#123;1..3&#125;.tsv&#x27;</span>, <span class="string">&#x27;TSV&#x27;</span>);</span><br><span class="line"><span class="comment">-- 这里写成 girls_?.tsv 也是可以的</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> girls_new (</span><br><span class="line">    id UInt32,</span><br><span class="line">    name String,</span><br><span class="line">    age UInt32,</span><br><span class="line">    place String</span><br><span class="line">) ENGINE <span class="operator">=</span> HDFS(<span class="string">&#x27;hdfs://localhost:6666/clickhouse/girls_?.tsv&#x27;</span>, <span class="string">&#x27;TSV&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p><strong>然后进行查询：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002654059-705168788.png" alt="img"></p>
<p><strong>显然使用 girls 和 girls_new 都是可以查询到数据的，由于是 3 个文件，因此会以 3 个分区的形式合并返回。</strong></p>
<p><strong>以上就是 HDFS 数据表的相关内容，可以看到使用起来还是非常方便的，但还是像我们之前说的那样，ClickHouse 完全独立于 Hadoop 生态圈，并不需要借助 HDFS 存储数据。但之所以还提供 HDFS 数据表，主要是为了读取 HDFS 文件系统上已存在的数据，不然的话我们需要先手动将数据从 HDFS 上下载下来，然后再导入到 ClickHouse 中，会比较麻烦，因此 ClickHouse 通过表引擎的形式直接支持我们访问 HDFS 文件系统。</strong></p>
<p><strong>当然不光是 HDFS，ClickHouse 还支持很多其它常见的外部存储系统，当然支持的目的都是为了读取这些存储系统中已存在的数据。</strong></p>
<h4 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h4><p><strong>MySQL 表引擎可以和 MySQL 数据库中的数据表建立映射，并通过 SQL 向其发起远程查询，包括 SELECT 和 INSERT，声明方式如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = MySQL(&#x27;host:port&#x27;, &#x27;database&#x27;, &#x27;table&#x27;, &#x27;user&#x27;, &#x27;password&#x27;[, replace_query, on_duplicate_clause])</span><br></pre></td></tr></table></figure>

<p><strong>假设我们要访问 MySQL 的 default 库下的 trade_info 表，那么可以这么做：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> clickhouse_trade_info (</span><br><span class="line">    id UInt32,</span><br><span class="line">    column1 type,</span><br><span class="line">    column2 type,</span><br><span class="line">    ......</span><br><span class="line">) ENGINE <span class="operator">=</span> MySQL(<span class="string">&#x27;localhost:3306&#x27;</span>, <span class="string">&#x27;default&#x27;</span>, <span class="string">&#x27;trade_info&#x27;</span>, <span class="string">&#x27;root&#x27;</span>, <span class="string">&#x27;123456&#x27;</span>)</span><br><span class="line"><span class="comment">-- 显然这几个参数的含义不需要多说，但还有两个可选参数 replace_query 和 on_duplicate_clause</span></span><br><span class="line"><span class="comment">-- replace_query 默认为 0，如果设置为 1，会用 REPLACE INTO 代替 INSERT INTO</span></span><br><span class="line"><span class="comment">-- on_duplicate_clause 默认为 0，对应 MySQL 的 ON DUPLICATE KEY 语法，如果想启用该设置，那么需要设置为 1</span></span><br></pre></td></tr></table></figure>

<p><strong>创建成功之后，我们就可以通过 ClickHouse 的数据表来读取 MySQL 数据表的数据了，当然插入数据也是可以的。由于 MySQL 还是比较简单的，这里就不实际演示了，可以自己测试一下。</strong></p>
<p><strong>当然重点是，我们可以搭配物化视图一起使用：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> MATERIALIZED <span class="keyword">VIEW</span> trade_info_view</span><br><span class="line">ENGINE <span class="operator">=</span> MergeTree()</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> id</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> clickhouse_trade_info</span><br><span class="line"><span class="comment">-- 这里指定数据表的时候一定要指定 ClickHouse 的数据表，不是 MySQL 的</span></span><br><span class="line"><span class="comment">-- 所以这里我们刻意将数据表其名为 clickhouse_trade_info</span></span><br></pre></td></tr></table></figure>

<p><strong>不过遗憾的是，目前 MySQL 表引擎不支持 UPDATE 和 DELETE 操作，如果需要数据更新的话，可以考虑使用 CollapsingMergeTree 作为视图的表引擎。不过还是之前所说，使用外部存储系统基本上都是为了读数据，很少会有插入、更新和删除之类的场景出现。</strong></p>
<h4 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h4><p><strong>相比 MySQL 表引擎，JDBC 表引擎不仅可以读取 MySQL 数据库，还能读取 PostgreSQL、SQLite 和 H2 数据库。但是光有 JDBC 表引擎还不够，它还需要依赖一个基于 Java 语言实现的 SQL 代理服务，名为 clickhouse-jdbc-bridge，它可以为 ClickHouse 代理访问数据库。</strong></p>
<p><strong>但 clickhouse-jdbc-bridge 需要使用 Maven 进行构建，而我本人不是 Java 方向的，只知道 Java 如何安装，甚至不知道如何用 Java 写一个 Hello World，所以更别提使用 Maven 构建项目了，因此这部分内容有兴趣可以自己了解一下。总之创建 JDBC 表引擎和 MySQL 表引擎是类似的：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = JDBC(&#x27;jdbc:url&#x27;, &#x27;database&#x27;, &#x27;table&#x27;)</span><br></pre></td></tr></table></figure>

<p><strong>不同的数据库使用不同的 url，可以自己测试一下。</strong></p>
<h4 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h4><p><strong>Kafka 是大数据领域非常流行的一款分布式消息系统，而 ClickHouse 也提供了 Kafka 表引擎与之对接，进而订阅 Kafka 中的主题并实时接收消息数据。而总所周知，在消息系统中存在三层语义：</strong></p>
<ul>
<li><code>最多一次（At Most Once）：可能出现消息丢失的情况，因为在这种情形下，一条消息在消费端最多被接收一次</code></li>
<li><code>最少一次（At Least Once）：可能出现消息重复的情况，因为在这种情形下，一条消息在消费端允许被接收多次</code></li>
<li><code>精确一次（Exactly Once）：数据不多不少，一条消息在消费端恰好被消费一次，这也是最理想的情况，因为消息不可能百分之百不丢失</code></li>
</ul>
<p><strong>虽然 Kafka 本身能够支持上述三种语义，但是目前 ClickHouse 还不支持精确一次语义，因为这需要应用端和 Kafka 深度配合才可以实现。kafka 使用 Offset 标志位来记录主题数据被消费的位置信息，当应用端接收到消息之后，通过自动提交或手动提交当前的位移信息，以保障消息的语义，但 ClickHouse 在这方面还有进步的空间。</strong></p>
<p><strong>Kafka 表引擎的声明方式如下：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ENGINE <span class="operator">=</span> Kafka()</span><br><span class="line">SETTINGS kafka_broker_list <span class="operator">=</span> <span class="string">&#x27;host:port,...&#x27;</span>,</span><br><span class="line">         kafka_topic_list <span class="operator">=</span> <span class="string">&#x27;topic1,topic2&#x27;</span>,</span><br><span class="line">         kafka_group_name <span class="operator">=</span> <span class="string">&#x27;group_name&#x27;</span>,</span><br><span class="line">         kafka_format <span class="operator">=</span> <span class="string">&#x27;data_format[,]&#x27;</span>,</span><br><span class="line">         [kafka_row_delimiter <span class="operator">=</span> <span class="string">&#x27;delimiter_symbol&#x27;</span>,]</span><br><span class="line">         [kafka_schema <span class="operator">=</span> <span class="string">&#x27;&#x27;</span>,]</span><br><span class="line">         [kafka_num_consumers <span class="operator">=</span> N,]</span><br><span class="line">         [kafka_skip_broken_message <span class="operator">=</span> N,]</span><br><span class="line">         [kafka_commit_every_batch <span class="operator">=</span> N]</span><br></pre></td></tr></table></figure>

<p><strong>其中带有方括号的表示选填项，下面依次介绍这些参数的作用：</strong></p>
<ul>
<li><strong>kafka_broker_list：表示 Broker 服务的地址列表，多个地址之间使用逗号分割</strong></li>
<li><strong>kafka_topic_list：表示订阅的消息主题的名称列表，多个主题之间使用逗号分割，多个主题中的数据均被消费</strong></li>
<li><strong>kafka_group_name：表示消费者组的名称，表引擎会依据此名称创建消费者组</strong></li>
<li><strong>kafka_format：表示用于解析消息的数据格式，在消息的发送端，必须按照此格式发送消息。而数据格式也必须是 ClickHouse 提供的格式之一，例如 TSV、JSONEachRow 和 CSV 等</strong></li>
<li><strong>kafka_row_delimiter：表示判定一行数据的结束符，默认为 ‘\0’</strong></li>
<li><strong>kafka_schema：对应 Kafka 的 schema 参数</strong></li>
<li><strong>kafka_num_consumers：表示消费者的数据量，默认值为 1，表引擎会依据此参数在消费者组中开启相应数量的消费者线程，当然线程数不要超过分区数，否则没有意义。因为在 kafka 的主题中，一个分区只能被某个消费者组里面的一个消费者消费（如果想被多个消费者消费，那么这些消费者一定要隶属于不同的消费者组）</strong></li>
<li><strong>kafka_skip_broken_message：当表引擎按照预定格式解析数据出现错误时，允许跳过失败的数据的行数，默认值为 0，即不允许任何格式错误的情形发生。在此种情形下，只要 kafka 主题中存在无法解析的数据，数据表都将不会接收任何数据。如果将其设置成非 0 的正整数，例如设置为 10，则表示只要 kafka 主题中存在无法解析的数据的总数小于 10，数据表就能正常接收消息数据，而解析错误的数据会被自动跳过</strong></li>
<li><strong>kafka_commit_every_batch：表示执行 kafka commit 的频率，因此这里提交偏移量的方式是手动提交，默认值为 0，即当一整个 Block 块完全写入数据表后才执行一次 commit。如果设置为 1，则每写完一个 Batch 批次的数据就会执行一次 kakfa commit（一次 Block 写入操作，由多次 Batch 写入操作而成）</strong></li>
</ul>
<p><strong>因此 ClickHouse 在对接 Kakfa 的时候是会将消息写入到数据表中的，所以还有一些配置参数可以调整表引擎的行为，比如 stream_poll_timeout_ms，它表示拉取数据的间隔时间。默认值为 500 毫秒，所以 Kafka 表引擎每隔 500 毫秒拉取一次数据，而拉取的数据会先被放入缓存当中，在时机成熟的时候，会被刷新到数据表。</strong></p>
<p><strong>而触发 Kakfa 表引擎刷新缓存的条件有两个，当满足其中任何一个时，便会触发刷新动作：</strong></p>
<ul>
<li><strong>当一个数据块写入完成的时候，一个数据块的大小由 kafka_max_block_size 参数控制，默认情况下大小为 65536</strong></li>
<li><strong>等待间隔超过 7500 毫秒，由 stream_fush_interval_ms 控制</strong></li>
</ul>
<p><strong>Kafka 表引擎底层负责和 Kafka 通信的部分是基于 librdkafka 实现的，这是一个由 C++ 实现的 Kafka 库，项目地址为 <a target="_blank" rel="noopener" href="https://github.com/edenhill/librdkafka">https://github.com/edenhill/librdkafka</a> 。librdkafka 提供了许多自定义的配置参数，例如在默认情况下，每次只会读取 kafka 中最新的数据，如果将 auto.offset.reset 改成 earliest（默认是 latest），数据将从会从最近一次提交的偏移位置开始读取。当然里面还支持很多其它的参数，可以通过项目中的 CONFIGURATION.md 进行查看。</strong></p>
<p><strong>ClickHouse 对 librdkafka 的自定义参数也提供了良好的扩展支持，在 ClickHouse 的全局设置中，提供了一组 Kafka 标签，专门用于定义 librdkafka 的自定义参数。不过需要注意的是，librdkafka 的原生参数中使用了点连接符，而在 ClickHouse 中需要改成下划线的形式，例如：</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">kafka</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- librdkafka 中，参数名是 auto.offset.reset，在这里需要使用下划线进行分割 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">auto_offset_reset</span>&gt;</span>earliest<span class="tag">&lt;/<span class="name">auto_offset_reset</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">kafka</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>下面我们就来测试一下，首先使用 Go 来连接 kafka，创建一个主题，并写入几条数据：</strong></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;encoding/json&quot;</span></span><br><span class="line">    <span class="string">&quot;github.com/Shopify/sarama&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    config := sarama.NewConfig()</span><br><span class="line">    cluster, _ := sarama.NewClusterAdmin([]<span class="type">string</span>&#123;<span class="string">&quot;47.94.174.89:9092&quot;</span>&#125;, config)</span><br><span class="line">    <span class="comment">// 创建主题，该主题有三个分区    </span></span><br><span class="line">    _ = cluster.CreateTopic(<span class="string">&quot;heroes&quot;</span>, &amp;sarama.TopicDetail&#123;NumPartitions: <span class="number">3</span>, ReplicationFactor: <span class="number">1</span>&#125;, <span class="literal">false</span>)</span><br><span class="line">    <span class="comment">// 写入消息，每个分区写入两条    </span></span><br><span class="line">    config.Producer.Return.Successes = <span class="literal">true</span></span><br><span class="line">    config.Producer.Return.Errors = <span class="literal">true</span></span><br><span class="line">    config.Producer.Partitioner = sarama.NewManualPartitioner</span><br><span class="line">    producer, _ := sarama.NewAsyncProducer([]<span class="type">string</span>&#123;<span class="string">&quot;47.94.174.89:9092&quot;</span>&#125;, config)</span><br><span class="line">    messages := []<span class="keyword">map</span>[<span class="type">string</span>]<span class="keyword">interface</span>&#123;&#125;&#123;</span><br><span class="line">        &#123;<span class="string">&quot;id&quot;</span>: <span class="number">1</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;麦克雷&quot;</span>, <span class="string">&quot;age&quot;</span>: <span class="number">37</span>, <span class="string">&quot;weapon&quot;</span>: <span class="string">&quot;维和者&quot;</span>, <span class="string">&quot;ultimate&quot;</span>: <span class="string">&quot;午时已到&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;id&quot;</span>: <span class="number">2</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;源氏&quot;</span>, <span class="string">&quot;age&quot;</span>: <span class="number">35</span>, <span class="string">&quot;weapon&quot;</span>: <span class="string">&quot;镖&quot;</span>, <span class="string">&quot;ultimate&quot;</span>: <span class="string">&quot;斩&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;id&quot;</span>: <span class="number">3</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;半藏&quot;</span>, <span class="string">&quot;age&quot;</span>: <span class="number">38</span>, <span class="string">&quot;weapon&quot;</span>: <span class="string">&quot;弓&quot;</span>, <span class="string">&quot;ultimate&quot;</span>: <span class="string">&quot;龙&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;id&quot;</span>: <span class="number">4</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;士兵76&quot;</span>, <span class="string">&quot;age&quot;</span>: <span class="number">55</span>, <span class="string">&quot;weapon&quot;</span>: <span class="string">&quot;脉冲步枪&quot;</span>, <span class="string">&quot;ultimate&quot;</span>: <span class="string">&quot;战术目镜&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;id&quot;</span>: <span class="number">5</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;死神(谐星)&quot;</span>, <span class="string">&quot;age&quot;</span>: <span class="number">57</span>, <span class="string">&quot;weapon&quot;</span>: <span class="string">&quot;*弹枪&quot;</span>, <span class="string">&quot;ultimate&quot;</span>: <span class="string">&quot;死亡绽放&quot;</span>&#125;,</span><br><span class="line">        &#123;<span class="string">&quot;id&quot;</span>: <span class="number">6</span>, <span class="string">&quot;name&quot;</span>: <span class="string">&quot;路霸&quot;</span>, <span class="string">&quot;age&quot;</span>: <span class="number">48</span>, <span class="string">&quot;weapon&quot;</span>: <span class="string">&quot;爆裂枪&quot;</span>, <span class="string">&quot;ultimate&quot;</span>: <span class="string">&quot;鸡飞狗跳&quot;</span>&#125;,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> i, message := <span class="keyword">range</span> messages &#123;</span><br><span class="line">        value, _ := json.Marshal(message)</span><br><span class="line">        <span class="comment">// 将 map 转成 json        </span></span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="number">2</span> &#123;</span><br><span class="line">            producer.Input() &lt;- &amp;sarama.ProducerMessage&#123;Topic: <span class="string">&quot;heroes&quot;</span>, Partition: <span class="number">0</span>,</span><br><span class="line">                Value: sarama.StringEncoder(value)&#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> i &lt; <span class="number">4</span> &#123;</span><br><span class="line">            producer.Input() &lt;- &amp;sarama.ProducerMessage&#123;Topic: <span class="string">&quot;heroes&quot;</span>, Partition: <span class="number">1</span>,</span><br><span class="line">                Value: sarama.StringEncoder(value)&#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            producer.Input() &lt;- &amp;sarama.ProducerMessage&#123;Topic: <span class="string">&quot;heroes&quot;</span>, Partition: <span class="number">2</span>,</span><br><span class="line">                Value: sarama.StringEncoder(value)&#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">select</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> &lt;-producer.Successes():</span><br><span class="line">        <span class="keyword">case</span> &lt;-producer.Errors():</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>以上我们就创建一个主题叫 heroes，该主题有三个分区，每个分区写入了两条数据。当然你也可以使用其它语言提供的 API 实现，下面我们通过 kafka 控制台查看一下数据有没有写入成功。</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002823891-539220090.png" alt="img"></p>
<p><strong>显然写入成功了，上面的 172.24.60.6 是我的内网 IP，然后我们就来创建 kafka 数据表获取数据。由于数据已经写入了，所以在读取的时候必须指定 auto.offset.reset 为 earliest。</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">kafka</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">auto_offset_reset</span>&gt;</span>earliest<span class="tag">&lt;/<span class="name">auto_offset_reset</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">kafka</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>我们修改 config.xml，然后 clickhouse restart 重启服务。下面开始创建 Kakfa 数据表：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_test (</span><br><span class="line">    id UInt32,    </span><br><span class="line">    name String,    </span><br><span class="line">    age UInt8,   </span><br><span class="line">    weapon String,   </span><br><span class="line">    ultimate String) </span><br><span class="line">ENGINE <span class="operator">=</span> Kafka() <span class="comment">-- 由于当前 ClickHouse 和 Kakfa 在同一个节点上，所以这里用内网 IP 也是可以的</span></span><br><span class="line">SETTINGS kafka_broker_list <span class="operator">=</span> <span class="string">&#x27;47.94.174.89:9092&#x27;</span>,         </span><br><span class="line">         kafka_topic_list <span class="operator">=</span> <span class="string">&#x27;heroes&#x27;</span>,         </span><br><span class="line">         kafka_group_name <span class="operator">=</span> <span class="string">&#x27;my_group&#x27;</span>,         </span><br><span class="line">         kafka_format <span class="operator">=</span> <span class="string">&#x27;JSONEachRow&#x27;</span>,         </span><br><span class="line">         kafka_num_consumers <span class="operator">=</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>

<p><strong>创建成功之后，我们来查询数据，看看能不能读取：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002832713-1985022276.png" alt="img"></p>
<p><strong>整体都很顺利，但问题是第二次查询的时候发现数据没了，原因就是 kafka 表引擎在执行完查询之后就会删除表内的数据。注意这里删除的数据是 Kakfa 表引擎从 kafka 中拖下来写入表中的数据，至于 kafka 上面的数据还在。不过很明显这不是我们期望的，因为不能每次查询都临时从 kafka 上拖吧。</strong></p>
<p><strong>所以真正的使用方式如下：</strong></p>
<ul>
<li><code>首先创建 Kafka 数据表 A，它充当的是数据管道，负责从 kafka 上拖数据</code></li>
<li><code>然后是另外一张任意引擎的数据表 B，它充当的角色是面向终端用户的查询表，在生产环境中通常是 MergeTree 系列</code></li>
<li><code>最后是一张物化视图 C，它负责将表 A 的数据实时同步到表 B</code></li>
</ul>
<p><strong>下面具体操作一波：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_queue (</span><br><span class="line">    id UInt32,</span><br><span class="line">    name String,</span><br><span class="line">    age UInt8,</span><br><span class="line">    weapon String,</span><br><span class="line">    ultimate String</span><br><span class="line">) ENGINE <span class="operator">=</span> Kafka()</span><br><span class="line">SETTINGS kafka_broker_list <span class="operator">=</span> <span class="string">&#x27;47.94.174.89:9092&#x27;</span>,</span><br><span class="line">         kafka_topic_list <span class="operator">=</span> <span class="string">&#x27;heroes&#x27;</span>,</span><br><span class="line">         kafka_group_name <span class="operator">=</span> <span class="string">&#x27;my_group&#x27;</span>,</span><br><span class="line">         kafka_format <span class="operator">=</span> <span class="string">&#x27;JSONEachRow&#x27;</span>,</span><br><span class="line">         kafka_num_consumers <span class="operator">=</span> <span class="number">3</span>;</span><br><span class="line">         </span><br><span class="line"><span class="comment">-- 然后是面向终端用户的查询表，这里使用 MergeTree 引擎</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> kafka_table (</span><br><span class="line">    id UInt32,</span><br><span class="line">    name String,</span><br><span class="line">    age UInt8,</span><br><span class="line">    weapon String,</span><br><span class="line">    ultimate String</span><br><span class="line">) ENGINE <span class="operator">=</span> MergeTree()</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 最后是一张物化视图，用于将数据从 kafka_queue 同步到 kafka_table</span></span><br><span class="line"><span class="keyword">CREATE</span> MATERIALIZED <span class="keyword">VIEW</span> queue_to_table_view <span class="keyword">TO</span> kafka_table</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span> id, name, age, weapon, ultimate <span class="keyword">FROM</span> kafka_queue</span><br></pre></td></tr></table></figure>

<p><strong>至此全部的工作就完成了，当数据进入 kafka_queue 的时候，物化视图 queue_to_table_view 会将数据从 kafka_queue 同步到 kafka_table，即使 kafka_queue 中的数据被删掉也不影响，因为数据已经进入了 kafka_table，而 kafka_table 才是负责面向数据查询的表。</strong></p>
<blockquote>
<p><strong>说白了数据删除的问题并没有得到本质上的解决，只是换了一种曲线救国的方式，通过物化视图将数据放在了另一张表中。至于 kafka 数据表在查询之后数据为什么被删除，我们就不深究了。</strong></p>
</blockquote>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002840055-450004326.png" alt="img"></p>
<blockquote>
<p><strong>注意：从 kafka 上面拖数据是有一定过程的，如果往 kafka 写完数据之后就立刻查询 kafka_table，不一定能查询得到数据，这之间会有一定的延迟。</strong></p>
</blockquote>
<p><strong>如果想停止数据同步，可以删除视图：DROP TABEL queue_to_table_view，或者卸载视图：DETACH TABLE queue_to_table_view。这里我们将视图卸载掉，然后再将之前的数据重新写入一次，并进行查询：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002845471-147057865.png" alt="img"></p>
<p><strong>我们发现数据并没有被同步过来，这是理所当然的，因为视图被卸载了。如果想继续同步，那么将卸载之后的视图重新装载进来即可：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ATTACH MATERIALIZED <span class="keyword">VIEW</span> queue_to_table_view <span class="keyword">TO</span> kafka_table</span><br><span class="line"><span class="keyword">AS</span> <span class="keyword">SELECT</span> id, name, age, weapon, ultimate <span class="keyword">FROM</span> kafka_queue</span><br></pre></td></tr></table></figure>

<p><strong>和创建视图类似，只需要将 CREATE 换成 ATTACH 即可，然后再进行查询：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002852594-1910988304.png" alt="img"></p>
<p><strong>此时数据就又同步过来了（如果没有同步过来就等一小会儿），但是切记：在重新装载物化视图之前一定不要查询 kakfa_queue，因为一旦查询数据就没了，物化视图就没法同步了。</strong></p>
<h4 id="File"><a href="#File" class="headerlink" title="File"></a>File</h4><p><strong>File 表引擎能够直接读取本地文件的数据，通常被作为一种扩展手段来使用，例如它可以读取由其它系统生成的数据文件，如果外部系统直接修改了文件，则变相达到了数据更新的目的；还可以将 ClickHouse 数据导出为本地文件；以及用于数据格式转换等场景。除此之外，File 表引擎也被应用于 clickhouse-local 工具，之前介绍过。</strong></p>
<p><strong>File 表引擎的声明方式如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = File(format)</span><br></pre></td></tr></table></figure>

<p><strong>其中 format 表示文件的数据格式，同样必须是 ClickHouse 支持的格式，例如 TSV、CSV、JSONEachRow 等。可以发现在 File 表引擎的定义参数中，并没有包含文件路径这一选项，因此 File 表引擎的数据文件只能保存在 config.xml 配置中由 path 指定的路径下，也就是和其它的数据表在同一个路径下。</strong></p>
<p><strong>每张 File 数据表均由目录和文件组成，其中目录以表的名称命名，而数据文件则以 data. 命名，比如 data.CSV、data.TSV 等等。而创建 File 表的方式有自动和手动两种，首先介绍自动创建的方式，即由 File 表引擎全权负责表目录和数据文件的创建：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> file_table (  </span><br><span class="line">    name String,    </span><br><span class="line">    <span class="keyword">value</span> UInt32</span><br><span class="line">) ENGINE <span class="operator">=</span> File(<span class="string">&#x27;CSV&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>和其它表引擎一样，当执行完上面的语句后，会在 &#x2F;var&#x2F;lib&#x2F;clickhouse&#x2F;data&#x2F;default 中创建相应的目录，但是里面还没有数据文件，我们接着写入数据：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> file_table <span class="keyword">VALUES</span> (<span class="string">&#x27;one&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;two&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;three&#x27;</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p><strong>在数据写入之后，file_table 下面便会生成一个 data.CSV 数据文件：</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@satori ~]<span class="comment"># cat /var/lib/clickhouse/data/default/file_table/data.CSV &quot;one&quot;,1&quot;two&quot;,2&quot;three&quot;,3</span></span><br><span class="line">[root@satori ~]<span class="comment"># </span></span><br></pre></td></tr></table></figure>

<p><strong>可以看到数据被写入了文件之中，但这种情况比较少见，因为写入数据我们基本上都不会使用外部存储系列的表引擎，它们存在的目的更多是为了读取现有的数据。所以接下来介绍手动创建的方式，也就是目录和里面的文件都已经存在了，它们是由 ClickHouse 之外的其它系统创建的，而我们需要使用 ClickHouse 读取它。</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002901308-37798833.png" alt="img"></p>
<p><strong>以上是一个文本文件，如果我们想要读取它该怎么做呢？首先要根据内部数据创建和合适表结构，这里我们应该选择 JSONEachRow：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> file_table_new (</span><br><span class="line">    id UInt32,    </span><br><span class="line">    name String,   </span><br><span class="line">    place String</span><br><span class="line">) </span><br><span class="line">ENGINE <span class="operator">=</span> File(<span class="string">&#x27;JSONEachRow&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>创建完之后，可以查询一下试试，不出意外是会报错的，原因就是 file_table_new 下面没有 data.JSONEachRow 文件。不同于 MergeTree，MergeTree 数据表创建完之后如果不写入数据，那么查询结果是空，并不会报错。但 File 表引擎，它要求目录下必须有相应的 data.format 文件，所以我们将刚才的 girls.txt 拷贝过去：</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@satori ~]<span class="comment"># cp girls.txt /var/lib/clickhouse/data/default/file_table_new/data.JSONEachRow</span></span><br></pre></td></tr></table></figure>

<p><strong>拷贝的时候记得重命名，文件必须叫 data.，拷贝之后再执行一下查询：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002907468-1401770015.png" alt="img"></p>
<p><strong>当然我们也可以继续向表中追加数据，都是没有问题的。这里可能会有人好奇，如果我们不建表，而是手动创建一个 file_table_new 目录，然后将文件拷贝过去可不可以呢。答案是不可以，因为一张表除了对应一个物理目录之外，还有部分的元信息，这些元信息是在创建表的时候产生的。所以一定要先建表，然后自动生成对应的目录之后，再将文件拷贝过去。</strong></p>
<p><strong>以上就是 File 表引擎的基础用法，可以看到 ClickHouse 想的还是比较周全的，为了已经存在的数据存储也提供了相应的接口。</strong></p>
<h3 id="内存类型"><a href="#内存类型" class="headerlink" title="内存类型"></a>内存类型</h3><p><strong>之前介绍的表引擎，它们都有一个共同的特点：数据是在磁盘中被访问的，而接下来我们会介绍几种内存类型的表引擎，数据会从内存中被直接访问。当然，虽然它们是内存表引擎，但并不意味着不支持物理存储（落盘），事实上除了 Memory 表引擎之外，其余的几款内存表引擎都会将数据写入磁盘，因为为了防止数据丢失，所以也提供了这种故障恢复手段。而在数据表被加载时，它们会将数据全部加载至内存，以供查询。而将数据全量放在内存中，显然是一把双刃剑，因为在提升查询性能的同时增大了内存消耗。</strong></p>
<h4 id="Memory"><a href="#Memory" class="headerlink" title="Memory"></a>Memory</h4><p><strong>Memory 表引擎直接将数据保存在内存中，数据既不会被压缩也不会被格式转换，数据在内存中保存的形态与查询时看到的如出一辙。正因为如此，当 ClickHouse 服务重启的时候，Memory 表内的数据会全部丢失。所以在一些场合，会将 Memory 作为测试表使用。由于不需要磁盘读取、序列化以及反序列化操作，所以 Memory 表引擎支持并行查询，并且在简单的查询场景中能够达到与 MergeTree 旗鼓相当的查询性能（一亿行数据以内）。Memory 表创建方法如下：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> sweet_memory_1 (id UInt64) ENGINE <span class="operator">=</span> Memory()</span><br></pre></td></tr></table></figure>

<p><strong>当数据被写入之后，磁盘上不会创建任何数据文件，如果服务重启，那么这张表就没了。比较简单，这里就不测试了，但最后需要说明的是，Memory 数据表不单单被用作测试，它还被广泛应用在 ClickHouse 的内部，它会作为集群间分发数据的存储载体来使用。例如在分布式 IN 查询的场合中，会利用 Memory 临时表保存 IN 字句的查询结果，并通过网络将它传输到远端节点，关于这部分内容后续介绍。</strong></p>
<h4 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h4><p><strong>Set 表引擎是拥有物理存储的，数据首先会被写入内存，然后被同步到磁盘文件中。所以服务重启之后它的数据不会丢失，当数据表被重新状态时，文件数据会再次被全量加载到内存。而 Set 我们知道它内部的数据是唯一的，对于有 Python 经验的人应该再熟悉不过了，也就是说 Set 表引擎具有数据去重的功能。在数据写入的过程中，重复的数据会被自动忽略。然而 Set 表引擎的使用场景即特殊又有限，它虽然支持正常的 INSERT 写入，但并不能直接使用 SELECT 进行查询，Set 表只能间接作为 IN 查询的右侧条件被查询使用。</strong></p>
<p><strong>Set 表引擎的存储结构由两部分组成，它们分别是：</strong></p>
<ul>
<li><strong>[num].bin 数据文件：保存了所有列字段的数据，其中 num 是一个自增 id，从 1 开始。伴随着每一批数据的写入（每一次 INSERT），都会生成一个新的 .bin 文件，num 也会随之加 1</strong></li>
<li><strong>tmp 临时目录：数据文件首先会被写到这个目录，当一批数据写入完毕之后，数据文件会被移出此目录</strong></li>
</ul>
<p><strong>下面就来创建一个 Set 数据表测试一下：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002915489-338948178.png" alt="img"></p>
<p><strong>正确的做法是将 Set 数据表作为 IN 查询的右侧条件，例如：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002921253-215978823.png" alt="img"></p>
<p><strong>再来查询一下 set_table 的物理目录结构：</strong></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@satori set_table]<span class="comment"># ls</span></span><br><span class="line">1.bin  tmp</span><br><span class="line">[root@satori set_table]<span class="comment">#</span></span><br></pre></td></tr></table></figure>

<p><strong>结果和我们分析的是一样的。</strong></p>
<h4 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h4><p><strong>Join 表引擎显然是为 JOIN 查询而生的，它等同于将 JOIN 查询进行了一层简单的封装。在 Join 表引擎的底层实现中，它与 Set 表引擎共用了大部分的处理逻辑，所以 Join 和 Set 表引擎拥有众多相似之处。例如 Join 表引擎的物理存储也由 [num].bin 数据文件和 tmp 临时目录两部分组成；数据首先会被写入内存，然后被同步到磁盘文件，但相比 Set 表引擎，Join 表引擎有着更加广泛的使用场景，它既能够作为 JOIN 查询的连接表，也能够被直接查询使用。</strong></p>
<p><strong>Join 表引擎的声明方式如下所示：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = Join(join_strictness, join_type, key1[, key2, ...])</span><br></pre></td></tr></table></figure>

<p><strong>其中各参数的含义如下：</strong></p>
<ul>
<li><code>join_strictness：连接精度，它决定了 JOIN 查询在连接数据时所使用的策略，目前支持 ALL、ANY、SEMI、ANTI 四种类型</code></li>
<li><code>join_type：连接类型，它决定了 JOIN 查询在组合左右两个数据集合的策略，目前支持 INNER、LEFT、RIGHT 和 FULL 四种类型</code></li>
<li><code>join_key：连接键，它决定了使用哪个列字段进行关联</code></li>
</ul>
<p><strong>上面这些参数，每一条都对应了 JOIN 查询字句的语法规则，关于 JOIN 查询后续展开，我们首先创建相关数据表测试一下：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 首先创建主表，引擎为 Log，关于 Log 数据表一会说</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> join_table (</span><br><span class="line">    id UInt8,</span><br><span class="line">    name String,</span><br><span class="line">    <span class="type">time</span> DateTime</span><br><span class="line">) ENGINE <span class="operator">=</span> <span class="built_in">Log</span>();</span><br><span class="line"><span class="comment">-- 写入数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> join_table</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">1</span>, <span class="string">&#x27;古明地觉&#x27;</span>, <span class="string">&#x27;2020-05-01 12:00:00&#x27;</span>),</span><br><span class="line">       (<span class="number">2</span>, <span class="string">&#x27;雾雨魔理沙&#x27;</span>, <span class="string">&#x27;2020-05-01 12:30:00&#x27;</span>),</span><br><span class="line">       (<span class="number">3</span>, <span class="string">&#x27;琪露诺&#x27;</span>, <span class="string">&#x27;2020-05-01 13:00:00&#x27;</span>);</span><br><span class="line">       </span><br><span class="line"><span class="comment">-- 接着创建 Join 表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> id_join_table (</span><br><span class="line">    id UInt8,</span><br><span class="line">    score UInt8,</span><br><span class="line">    <span class="type">time</span> DateTime</span><br><span class="line">) ENGINE <span class="operator">=</span> <span class="keyword">Join</span>(<span class="keyword">ANY</span>, <span class="keyword">LEFT</span>, id);</span><br><span class="line"><span class="comment">-- 如果 join_strictness 为 ANY，那么 join_key 重复的数据会自动被忽略</span></span><br><span class="line"><span class="comment">-- 所以下面虽然写了两条 id 为 1 数据，但只有第一条会保留，因为写入第二条的时候发现 id 为 1 的数据已经存在了，所以会停止写入</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> id_join_table</span><br><span class="line"><span class="keyword">VALUES</span> (<span class="number">1</span>, <span class="number">100</span>, <span class="string">&#x27;2020-05-01 11:55:00&#x27;</span>),</span><br><span class="line">       (<span class="number">1</span>, <span class="number">105</span>, <span class="string">&#x27;2020-05-01 11:10:00&#x27;</span>),</span><br><span class="line">       (<span class="number">2</span>, <span class="number">90</span>, <span class="string">&#x27;2020-05-01 12:01:00&#x27;</span>),</span><br><span class="line">       (<span class="number">3</span>, <span class="number">80</span>, <span class="string">&#x27;2020-05-01 13:10:00&#x27;</span>),</span><br><span class="line">       (<span class="number">5</span>, <span class="number">70</span>, <span class="string">&#x27;2020-05-01 14:00:00&#x27;</span>),</span><br><span class="line">       (<span class="number">6</span>, <span class="number">60</span>, <span class="string">&#x27;2020-05-01 13:50:00&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p><strong>我们查询一下：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002929823-522994560.png" alt="img"></p>
<p><strong>我们看到是可以查询成功的，Join 数据表支持查询，但这种查询方式并不是 Join 数据表的主战场，它的主战场应该是 Join 查询，例如：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002935024-890100815.png" alt="img"></p>
<p><strong>当然 Join 数据表除了可以直接使用 SELECT 和 JOIN 之外，还可以通过 join 函数访问：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002941690-331743065.png" alt="img"></p>
<p><strong>目前还没有涉及到 JOIN 查询，所以一些细节我们还没有解释，目前只需要知道有这么个引擎即可，具体内容在后面介绍查询的时候再详细说。</strong></p>
<h4 id="Buffer"><a href="#Buffer" class="headerlink" title="Buffer"></a>Buffer</h4><p><strong>Buffer 表引擎完全使用内存装载数据，不支持文件的持久化机制，所以当服务重启之后，表内的数据会被清空。Buffer 表引擎不是为了面向查询场景而设计的，它的作用是充当缓冲区的角色。假设有这样一种场景，我们需要将数据写入目标 MergeTree 表 A，由于写入的并发数很高，这可能导致表 A 的合并速度慢于写入速度，因为每次 INSERT 都会生成一个新的分区目录。此时便可引入 Buffer 数据表来缓解这类问题，将 Buffer 表作为数据写入的缓冲区，数据首先会被写入 Buffer 表，当满足预设条件时，Buffer 表会自动将数据刷新到目标表。</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002949593-1004963799.png" alt="img"></p>
<p><strong>Buffer 表引擎的声明方式如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = Buffer(database, table, num_layers, min_time, max_time, min_rows, max_rows, min_bytes, max_bytes)</span><br></pre></td></tr></table></figure>

<p><strong>里面参数的作用如下：</strong></p>
<ul>
<li><code>database：目标表的数据库</code></li>
<li><code>table：目标表，Buffer 表内的数据会自动刷新到目标表</code></li>
<li><code>num_layers：可以理解为线程数，Buffer 表会按照 num_layers 的数量开启线程，以并行的方式将数据刷新到目标表，官方建议设置为 16</code></li>
</ul>
<p><strong>Buffer 表并不是实时刷新数据的，只有在阈值条件满足时才会刷新，阈值条件由三个最小和最大值组成，含义如下：</strong></p>
<ul>
<li><code>min_time 和 max_time：时间条件的最小值和最大值，单位为秒，从第一次向表内写入数据时开始计算</code></li>
<li><code>min_rows 和 max_rows：数据行数条件的最小值和最大值</code></li>
<li><code>min_bytes 和 max_bytes：数据大小的最小值和最大值，单位为字节</code></li>
</ul>
<p><strong>针对以上条件，Buffer 表刷新数据的判断依据有三个，满足其中任意一个就会刷新数据：</strong></p>
<ul>
<li><code>三组条件中所有最小阈值都已满足，则触发刷新动作</code></li>
<li><code>三组条件中有一个最大阈值满足（这里是超过最大值），则触发刷新动作</code></li>
<li><code>如果写入一批数据的行数大运 max_rows 或者数据大小大于 max_bytes，则数据直接写入目标表</code></li>
</ul>
<p><strong>还有一点需要注意，上述三组条件在每一个 layer 中都是单独的计算的，假设 num_layers 为 16，则 Buffer 表最多开启 16 个线程来响应数据的写入，它们以轮训的方式接收请求，在每个线程内会独立进行上述判断的过程。也就是说，假设一张 Buffer 表的 max_bytes 为 100 MB，num_layers 为 16，那么这张 Buffer 表能够同时处理的最大数据量约为 1600 MB。</strong></p>
<p><strong>下面来测试一下它的用法，首先创建一个 Memory 数据表，再创建一张 Buffer 数数据表：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> memory_table (id UInt64) ENGINE <span class="operator">=</span> Memory();</span><br><span class="line"><span class="comment">-- 创建 Buffer 表，用于往 Memory 表里面写入数据</span></span><br><span class="line"><span class="comment">-- 注意这里 Buffer 表的创建语法，后面必须要有 AS，并且这里的 AS 还不是别名的意思</span></span><br><span class="line"><span class="comment">-- 因为创建 Buffer 数据表的时候我们没有指定字段，那么这张表的结构长什么样呢？不用想肯定和 Memory 数据表一样</span></span><br><span class="line"><span class="comment">-- 因为数据就是要往它里面导，所以 AS memory_table 表示将表 memory_table 的结构作为表 buffer_to_memory_table 的结构</span></span><br><span class="line"><span class="comment">-- 当然除了 memory_table 还可以是其它的数据表，不过一般和目标表保持一致，因为数据就是要刷到目标表里面的</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> buffer_to_memory_table <span class="keyword">AS</span> memory_table</span><br><span class="line">ENGINE <span class="operator">=</span> Buffer(<span class="keyword">default</span>, memory_table, <span class="number">16</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">10000</span>, <span class="number">1000000</span>, <span class="number">10000000</span>, <span class="number">100000000</span>);</span><br><span class="line"><span class="comment">-- 接下来向 Buffer 表里面写入 100 万行数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> buffer_to_memory_table <span class="keyword">SELECT</span> number <span class="keyword">FROM</span> numbers(<span class="number">1000000</span>);</span><br></pre></td></tr></table></figure>

<p><strong>此时 buffer_to_memory_table 内部有数据，但 memory_table 里面没有，因为三个条件，没有一个达到最大阈值（准确的说是超过）。而在 100 秒之后才会有数据，可以验证一下：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901002957742-783985999.png" alt="img"></p>
<p><strong>然后我们再写入一批数据，此时数据量改为 100 万零 1 条：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901003004810-426340530.png" alt="img"></p>
<p><strong>可以看到此时不会等待，Buffer 会立即将数据写入目标表。</strong></p>
<h3 id="日志类型"><a href="#日志类型" class="headerlink" title="日志类型"></a>日志类型</h3><p><strong>如果使用的数据量很小（例如 100 万以下），面对的数据查询场景也比较简单，并且是一次写入多次查询的模式，那么使用日志家族系列的表引擎将会是一种不错的选择。与合并树家族表引擎类似，日志家族系列的表引擎也有一些共性特征：比如均不支持索引、分区等高级特性，不支持并发读写等等。当针对一张日志表写入数据时，针对这张表的查询会被阻塞，直至写入动作结束。但它们同时也拥有切实的物理存储，数据会被保存到本地文件中，当然除了这些共同的特征职位啊啊，日志家族系列的表引擎也有这各自的特点。接下来就从性能由低到高的顺序，一依次介绍这些表引擎的使用方法。</strong></p>
<h4 id="TinyLog"><a href="#TinyLog" class="headerlink" title="TinyLog"></a>TinyLog</h4><p><strong>TinyLog 是日志家族中性能最低的表引擎，它的存储结构由数据文件和元数据两部分组成。其中数据文件是按列存储的，也就是说每个字段都有与之对应的 .bin 文件，这种结构和 MergeTree 有些相似，但 TinyLog 既不支持分区，也没有 .mrk 标记文件。由于没有标记文件，它自然无法支持 .bin 文件的并行读取操作，所以它只适合在非常简单的场景下使用。下面就来创建一张 TinyLog 数据表：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tiny_log_table (</span><br><span class="line">    id UInt64,   </span><br><span class="line">    code UInt64</span><br><span class="line">) ENGINE <span class="operator">=</span> TinyLog(); </span><br><span class="line"><span class="comment">-- 接着写入数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> tiny_log_table <span class="keyword">SELECT</span> number, number <span class="operator">+</span> <span class="number">1</span> <span class="keyword">FROM</span> numbers(<span class="number">100</span>)</span><br></pre></td></tr></table></figure>

<p><strong>数据写入之后就能通过 SELECT 语句对它进行查询了，这里就不展示查询结果了，都能想到是什么，我们来看一下物理存储：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901003011837-135493468.png" alt="img"></p>
<p><strong>可以看到 id 和 code 各自生成了对应的 .bin 数据文件，然后还有一个 sizes.json，里面通过 JSON 格式记录了每个 .bin 文件内对应的数据大小信息。</strong></p>
<h4 id="StripeLog"><a href="#StripeLog" class="headerlink" title="StripeLog"></a>StripeLog</h4><p><strong>StripeLog 表引擎的存储结构由固定的三个文件组成，分别是：</strong></p>
<ul>
<li><code>data.bin：数据文件，所有的列字段使用同一个文件保存，所有数据均会写入 data.bin，类似于数据量没超过阈值的 MergeTree 表</code></li>
<li><code>index.mrk：数据标记，保存了数据在 data.bin 文件中的位置信息，利用数据标记能够使用多个线程以并行的方式读取 data.bin 内的压缩数据块，从而提升数据查询的性能</code></li>
<li><code>sizes.json：元数据文件，记录了 data.bin 和 index.mrk 大小的信息</code></li>
</ul>
<p><strong>从上述信息能够得知，相比 TinyLog 而言，StripeLog 拥有更高的查询性能（因为具有 .mrk 文件，支持并行查询），同时其使用了更少的文件描述符（所有列都使用同一个文件保存）。下面来创建 StripeLog 数据表：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> stripe_log_table ( </span><br><span class="line">    id UInt64,    </span><br><span class="line">    code UInt64</span><br><span class="line">) ENGINE <span class="operator">=</span> StripeLog();</span><br><span class="line"><span class="comment">-- 然后写入数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> stripe_log_table <span class="keyword">SELECT</span> number, number <span class="operator">+</span> <span class="number">100</span> <span class="keyword">FROM</span> numbers(<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<p><strong>数据写入之后即可进行查询，这里我们还是直接查看一下物理存储目录：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901003018972-1358099321.png" alt="img"></p>
<p><strong>里面只有三个文件，其代表的含义显然无需解释了。</strong></p>
<h4 id="Log"><a href="#Log" class="headerlink" title="Log"></a>Log</h4><p><strong>Log 表引擎结合了 TinyLog 和 StripeLog 两个表引擎的长处，是日志家族系列中性能最高的表引擎，Log 表引擎的存储结构由 3 个部分组成：</strong></p>
<ul>
<li><code>[column].bin：数据文件，数据文件按列独立存储，每一个列字段都拥有一个与之对应的 .bin 文件</code></li>
<li><code>__marks.mrk：数据标记，统一保存了数据在各个 [column].bin 文件中的位置信息，利用数据标记能够使用多个线程以并行的方式读取 [column].bin 内的压缩数据块，从而提升数据查询的性能</code></li>
<li><code>sizes.json：元数据文件，记录了 [column].bin 和 __marks.mrk 大小的信息</code></li>
</ul>
<p><strong>下面创建 Log 数据表：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> log_table (  </span><br><span class="line">    id UInt64,    </span><br><span class="line">    code UInt64</span><br><span class="line">) ENGINE <span class="operator">=</span> <span class="built_in">Log</span>();</span><br><span class="line"><span class="comment">-- 然后写入数据</span></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> log_table <span class="keyword">SELECT</span> number, number <span class="operator">+</span> <span class="number">100</span> <span class="keyword">FROM</span> numbers(<span class="number">1000</span>)</span><br></pre></td></tr></table></figure>

<p><strong>数据写入之后即可进行查询，相信都能看出 TinyLog、StripeLog、Log 之间是高度相似的，我们还是只看一下目录结构：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901003025726-1365104681.png" alt="img"></p>
<p><strong>以上就是日志类型的表引擎，个人觉得算是最简单的了，甚至比内存类型的表引擎还要简单。</strong></p>
<h3 id="接口类型"><a href="#接口类型" class="headerlink" title="接口类型"></a>接口类型</h3><p><strong>有这么一类表引擎，它们自身并不存储任何数据，而是像粘合剂一样可以整合其它的数据表。在使用这类表引擎的时候，我们不用担心底层的复杂性，它们就像接口一样，为用户提供了统一的访问界面，所以将它们归为接口类表引擎。</strong></p>
<h4 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h4><p><strong>假设有这样一种场景：在数据仓库的设计中，数据按年分表存储，例如 test_table_2018、test_table_2019 和 test_table_2020，但是现在需要跨年度查询这些数据，应该如何实现呢？在这种情形下，使用 Merge 表引擎就是一种很合适的选择了。</strong></p>
<p><strong>Merge 表引擎就如同一层使用了门面模式的代理，它本身不存储任何数据，也不支持数据写入，它的作用就如同它的名字，即负责合并多个查询结果集。Merge 表引擎可以代理查询任意数量的数据表，这些查询会异步且并行执行，并最终合并成一个结果集返回。被代理查询的数据表被要求处于同一个数据库内，且拥有相同的表结构，但它们可以使用不同的表引擎以及不同的分区定义（对于 MergeTree 而言）。</strong></p>
<p><strong>Merge 表引擎的声明方式如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = Merge(database, table_name)</span><br></pre></td></tr></table></figure>

<p><strong>其中 database 为数据库的名称，table_name 为数据表的名称，它支持使用正式则表达式，比如 ^ 表示合并所有以 test 为前缀的数据表。下面我们来简单说明一下 Merge 的使用方法：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- test_table_2018 保存了 2018 年的数据</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table_2018 (</span><br><span class="line">    id String,</span><br><span class="line">    create_time DateTime,</span><br><span class="line">    code String</span><br><span class="line">) ENGINE <span class="operator">=</span> MergeTree()</span><br><span class="line"><span class="keyword">PARTITION</span> <span class="keyword">BY</span> toYYYYMM(create_time)</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> id;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 然后是 test_table_2019，它的结构和 test_table_2018 相同，但使用了不同的表引擎</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_table_2019 (</span><br><span class="line">    id String,</span><br><span class="line">    create_time DateTime,</span><br><span class="line">    code String</span><br><span class="line">) ENGINE <span class="operator">=</span> <span class="built_in">Log</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 然后创建 Merge 表将上面两张表结合，这里的 AS 相当于为 merge_test_table_2018_and_2019 指定表结构</span></span><br><span class="line"><span class="comment">-- 显然这里会复制 test_table_2018 的表结构</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> merge_test_table_2018_and_2019 <span class="keyword">AS</span> test_table_2018</span><br><span class="line">ENGINE <span class="operator">=</span> <span class="keyword">Merge</span>(currentDatabase(), <span class="string">&#x27;^test_table_201&#x27;</span>)</span><br><span class="line"><span class="comment">-- currentDatabase() 表示获取当前的数据库</span></span><br></pre></td></tr></table></figure>

<p><strong>然后我们来写入一些数据，然后进行查询：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901003033723-71170041.png" alt="img"></p>
<p><strong>通过返回的结果集可以印证，所有以 test_table_201 开头的数据表都被分别查询，然后合并返回了。值得一提的是，对于 Merge 数据表而言，会有一个虚拟字段 _table，它表示某行数据的来源表。所以通过 _table 我们可以实现表的过滤，比如我们新创建了 test_table_2017 表示 2017 的数据，但当前我们并且不需要 2017 的数据，那么就可以将其作为查询条件给过滤掉，比如：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> merge_test_table_2018_and_2019 <span class="keyword">WHERE</span> _table <span class="operator">!=</span> <span class="string">&#x27;test_table_2017&#x27;</span></span><br></pre></td></tr></table></figure>

<p><strong>还是蛮方便的，此时 _table 将等同于索引，Merge 表忽略那些被排除在外的表，不会向他们发起查询请求。</strong></p>
<h4 id="Dictionary"><a href="#Dictionary" class="headerlink" title="Dictionary"></a>Dictionary</h4><p><strong>这里涉及到了数据字典，关于数据字典我们会专门放在后面说，目前可以先了解一下。Dictionary 表引擎是数据字典的一层代理封装，它可以取代字典函数，让用户通过数据表查询字典。字典内的数据被加载后，会全部保存到内存中，所以使用 Dictionary 对字典性能没有任何影响。声明 Dictionary 数据表的方式如下：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = Dictionary(dict_name)</span><br></pre></td></tr></table></figure>

<p><strong>其中 dict_name 对应一个已被加载的字典名称，举个栗子：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> tb_test_flat_dict ( </span><br><span class="line">    id UInt64,    </span><br><span class="line">    code String,  </span><br><span class="line">    name String</span><br><span class="line">) ENGINE <span class="operator">=</span> Dictionary(test_flat_dict)  </span><br></pre></td></tr></table></figure>

<p><strong>tb_test_flat_dict 等同于数据字典 test_flat_dict 的代理表，对它进行 SELECT 查询即可获取内部的数据。</strong></p>
<h4 id="Distributed"><a href="#Distributed" class="headerlink" title="Distributed"></a>Distributed</h4><p><strong>在数据库领域，当面对海量业务数据的时候，一种主流的做法是实施 Sharding 方案，即将一张数据表横向扩展到多个数据库实例。其中每个数据库实例称为一个 Shard 分片，数据在写入时，需要按照预定的业务规则均匀地写至各个 Shard 分片；而在数据查询时，则需要在每个 Shard 分片上分别查询，最后归并结果集。所以为了实现 Sharding 方案，一款支持分布式数据库的中间件是必不可少的，例如 Apache ShardingSphere。</strong></p>
<p><strong>ClickHouse 作为一款性能卓越的分布式数据库，自然也是支持 Sharding 方案的，而 Distributed 表引擎就等同于 Sharding 方案中的数据库中间件。Distributed 表引擎自身不存储任何数据，它能够作为分布式表的一层透明代理，在集群内部自动开展数据的写入分发以及查询路由工作。关于 Distributed 表引擎的详细介绍，将会在后续展开。</strong></p>
<h3 id="其它类型"><a href="#其它类型" class="headerlink" title="其它类型"></a>其它类型</h3><p><strong>接下来将要介绍的几款表引擎，由于各自用途迥异，所以只好把它们归为其它类型。最然这些表引擎的使用场景并不广泛，但仍建议了解它们的特性和使用方法，因为这些表引擎扩充了 ClickHouse 的能力边界。在一些特殊的场合，它们也能够发挥重要作用。</strong></p>
<h4 id="Live-View"><a href="#Live-View" class="headerlink" title="Live View"></a>Live View</h4><p><strong>虽然 ClickHouse 已经提供了准实时的数据处理手段，例如 Kafka 表引擎和物化视图，但是在应用层面，一直缺乏开放给用户的事件监听机制。所以从 19.14 版本开始，ClickHouse 提供了一种全新的视图：Live View。</strong></p>
<p><strong>Live View 是一种特殊的视图，虽然它并不属于表引擎，但是因为它与数据表息息相关，所以还是把 LiveView 归类到了这里。Live View 的作用类似事件监听器，它能够将一条 SQL 查询结果作为监控目标，当目标数据增加时，LiveView 可以及时发出响应。若要使用 Live View，首先需要将 allow_experimental_live_view 参数设置为 1，可以执行如下语句确认参数是否设置正确：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901003041543-1642442471.png" alt="img"></p>
<p><strong>现在来举例说明，首先创建一张数据表，它将作为 Live View 的监听目标：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> origin_table (</span><br><span class="line">    id UInt64</span><br><span class="line">) ENGINE <span class="operator">=</span> <span class="built_in">Log</span>();</span><br><span class="line"><span class="comment">-- 紧接着创建一个 Live View</span></span><br><span class="line"><span class="keyword">CREATE</span> LIVE <span class="keyword">VIEW</span> lv_origin <span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="built_in">COUNT</span>(<span class="operator">*</span>) <span class="keyword">FROM</span> origin_table;</span><br><span class="line"><span class="comment">-- 然后执行 watch 命令开启监听模式</span></span><br><span class="line"><span class="comment">-- 以后每当 origin_table 中的数据发生变化，就会执行 SELECT COUNT(*)</span></span><br><span class="line">WATCH lv_origin;</span><br></pre></td></tr></table></figure>

<p><strong>如此一来 Live View 就进入监听模式了，首先 origin_table 里面是没有数据的，所以显示结果为 0：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901003047770-2122133580.png" alt="img"></p>
<p><strong>然后再开启一个客户端，向 origin_table 里面写入数据，假设写入 10 条。数据写入之后会发现 Live View 做出了实时响应，查询的值变成了 10，并且虚拟字段 _version 会伴随着每一次的响应增加 1。</strong></p>
<h4 id="Null"><a href="#Null" class="headerlink" title="Null"></a>Null</h4><p><strong>Null 表引擎的功能与作用，与 Unix 系统的空设备 &#x2F;dev&#x2F;null 很相似，如果用户向 Null 表写入数据，系统会正确返回，但数据会被 Null 表自动忽略，永远不会将它们保存。如果用户向 Null 表发起查询，那么它将返回空。在使用物化视图的时候，如果不希望保留源表的数据，那么将源表设置成 Null 引擎将会是非常好的选择。下面就来举个栗子：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 首先创建一张 Null 表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> null_table (id UInt8) ENGINE <span class="operator">=</span> <span class="keyword">Null</span>();</span><br><span class="line"><span class="comment">-- 接着以 null_table 为源表，建立一张物化视图</span></span><br><span class="line"><span class="keyword">CREATE</span> MATERIALIZED <span class="keyword">VIEW</span> view_table</span><br><span class="line">ENGINE <span class="operator">=</span> TinyLog() <span class="keyword">AS</span> <span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> null_table</span><br></pre></td></tr></table></figure>

<p><strong>如果往 null_table 里面写数据，那么数据会被顺利同步到 view_table 中，但是 null_table 中是查询不到数据的。</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901003056841-1947455603.png" alt="img"></p>
<h4 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h4><p><strong>URL 表引擎的作用等价于 HTTP 客户端，它可以通过 HTTP&#x2F;HTTPS 协议，直接访问远端的 REST 服务。当执行 SELECT 查询的时候，底层会将其转换为 GET 请求的远程调用；而执行 INSERT 查询的时候，会将其转成 POST 请求的远程调用，并将数据以字节流的形式传递。URL 表引擎的声明方式如下所示：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ENGINE = URL(&#x27;url&#x27;, format)</span><br></pre></td></tr></table></figure>

<p><strong>其中 url 表示远端的服务地址，而 format 则是 ClickHouse 支持的数据格式，如 TSV、CSV 和 JSON 等。</strong></p>
<p><strong>这里我们用 Python 的 FastAPI 编写一个 web 服务：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, Request, Response</span><br><span class="line"><span class="keyword">import</span> orjson</span><br><span class="line"><span class="keyword">import</span> uvicorn</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line">table = []</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">&quot;/girls&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">get</span>():</span><br><span class="line">    response = Response(orjson.dumps(table),</span><br><span class="line">                        media_type=<span class="string">&quot;application/json&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">&quot;/girls&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">post</span>(<span class="params">request: Request</span>):</span><br><span class="line">    <span class="comment"># 如果插入多行数据，那么这些数据之间会以 \n 进行分割</span></span><br><span class="line">    data = re.split(<span class="string">rb&quot;(?&lt;=&#125;)\n(?=&#123;)&quot;</span>, <span class="keyword">await</span> request.body())</span><br><span class="line">    rows = [orjson.loads(_) <span class="keyword">for</span> _ <span class="keyword">in</span> data]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(rows, <span class="built_in">dict</span>):</span><br><span class="line">        table.append(rows)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        table.extend(rows)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    uvicorn.run(<span class="string">&quot;main:app&quot;</span>, host=<span class="string">&quot;0.0.0.0&quot;</span>, port=<span class="number">5555</span>)</span><br></pre></td></tr></table></figure>

<p><strong>启动之后我们来创建表：</strong></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> url_table (</span><br><span class="line">    id UInt32,</span><br><span class="line">    name String,</span><br><span class="line">    place String</span><br><span class="line">) ENGINE <span class="operator">=</span> URL(<span class="string">&#x27;http://localhost:5555/girls&#x27;</span>, JSONEachRow)</span><br></pre></td></tr></table></figure>

<p><strong>以后每执行一次 SELECT 都相当于发起了一次 GET 请求，执行一次 INSERT 相当于发起了一次 POST 请求，我们来测试一下：</strong></p>
<p><img src="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/1229382-20210901003104903-1326901560.png" alt="img"></p>
<p><strong>可以看出 ClickHouse 想的真是无比周到，考虑了大量的数据源，以上就是其它表引擎的全部内容。至于 Dictionary 和 Distributed 两个表引擎我们后面再说，因为涉及了还没介绍的内容。</strong></p>
<p><strong>到目前为止我们知道了除了 MergeTree 家族表引擎之外还有另外 5 种表引擎，这些表引擎丰富了 ClickHouse 的使用场景，扩充了 ClickHouse 的使用界限。下面再总结一下：</strong></p>
<ul>
<li><strong>外部存储类型的表引擎和 Hive 的外部表很类似，它们只负责元数据的管理和数据查询，自身并不负责数据的生成，数据文件直接由外部系统维护。它们可以直接读取 HDFS、本地文件、常见关系型数据库以及 Kafka 的数据。</strong></li>
<li><strong>内存类型的表引擎中的数据是常驻内存的，所以它们拥有堪比 MergeTree 的查询性能（1 亿数据量以内），其中 Set 和 Join 表引擎拥有物理存储，数据在写入内存的同时也会被刷到磁盘；而 Memory 和 Buffer 表引擎在服务重启之后，数据便会被清空。内存类表引擎是一把双刃剑，在数据大于 1 亿的场景下不建议使用内存类型的表引擎。</strong></li>
<li><strong>接口类型的表引擎自身并不存储任何数据，而是像粘合剂一样可以整合其它的数据表，其中 Merge 表引擎能够合并查询任意两张表结构相同的数据表；Dictionary 表引擎能够代理查询数据字典；而 Distributed 表引擎的作用类似分布式数据库的分表中间件，能够帮助用户简化数据的分发和路由工作。</strong></li>
<li><strong>其它类型的表引擎用途各不相同，其中 Live View 是一种特殊的视图，能够对 SQL 查询进行实时监听；Null 表引擎类似于 Linux 系统的空设备 &#x2F;dev&#x2F;null，通常和物化视图一起搭配使用；而 URL 表引擎类似于 HTTP 客户端，能够代理调用远端的 REST 服务。</strong></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://hesay.cn">wenwen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://hesay.cn/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/">https://hesay.cn/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://hesay.cn" target="_blank">热心市民温温</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ClickHouse/">ClickHouse</a></div><div class="post_share"><div class="social-share" data-image="/img/favicon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2023/04/11/ClickHouse%20%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0(%E5%8D%81%E4%B8%80)/" title="ClickHouse 中的常用聚合函数(十一)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ClickHouse 中的常用聚合函数(十一)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/04/11/ClickHouse%20%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0(%E5%8D%81%E4%B8%80)/" title="ClickHouse 中的常用聚合函数(十一)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">ClickHouse 中的常用聚合函数(十一)</div></div></a></div><div><a href="/2023/04/11/ClickHouse%20%E4%B8%AD%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E%EF%BC%9AMergeTree%20%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90(%E5%85%AD)/" title="ClickHouse 中最重要的表引擎：MergeTree 的深度原理解析(六)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">ClickHouse 中最重要的表引擎：MergeTree 的深度原理解析(六)</div></div></a></div><div><a href="/2023/04/11/ClickHouse%20%E4%B9%8B%20MergeTree%20%E5%AE%B6%E6%97%8F%E4%B8%AD%E7%9A%84%E5%85%B6%E5%AE%83%E8%A1%A8%E5%BC%95%E6%93%8E(%E4%B8%83)/" title="ClickHouse 之 MergeTree 家族中的其它表引擎(七)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">ClickHouse 之 MergeTree 家族中的其它表引擎(七)</div></div></a></div><div><a href="/2023/04/11/ClickHouse%20%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E4%BB%A5%E5%8F%8A%E5%90%84%E7%A7%8D%E5%AD%90%E5%8F%A5(%E4%B9%9D)/" title="ClickHouse 中的数据查询以及各种子句(九)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">ClickHouse 中的数据查询以及各种子句(九)</div></div></a></div><div><a href="/2023/04/11/ClickHouse%20%E5%85%B6%E5%AE%83%E7%9A%84%E4%B8%80%E4%BA%9B%E6%93%8D%E4%BD%9C%E5%87%BD%E6%95%B0%20(%E5%8D%81%E4%BA%94)/" title="ClickHouse 其它的一些操作函数 (十五)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">ClickHouse 其它的一些操作函数 (十五)</div></div></a></div><div><a href="/2023/04/11/ClickHouse%20%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C%E5%87%BD%E6%95%B0(%E5%8D%81%E4%BA%8C)/" title="ClickHouse 字符串的相关操作函数(十二)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-11</div><div class="title">ClickHouse 字符串的相关操作函数(十二)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/favicon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">wenwen</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wenlinshan"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">码海无涯。。。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ClickHouse-%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E-%E5%85%AB"><span class="toc-number">1.</span> <span class="toc-text">ClickHouse 中其它常见的表引擎(八)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A5%94%E5%AD%90"><span class="toc-number">1.0.1.</span> <span class="toc-text">楔子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%96%E9%83%A8%E5%AD%98%E5%82%A8%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.0.2.</span> <span class="toc-text">外部存储类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HDFS"><span class="toc-number">1.0.2.1.</span> <span class="toc-text">HDFS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MySQL"><span class="toc-number">1.0.2.2.</span> <span class="toc-text">MySQL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#JDBC"><span class="toc-number">1.0.2.3.</span> <span class="toc-text">JDBC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kafka"><span class="toc-number">1.0.2.4.</span> <span class="toc-text">Kafka</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#File"><span class="toc-number">1.0.2.5.</span> <span class="toc-text">File</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.0.3.</span> <span class="toc-text">内存类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Memory"><span class="toc-number">1.0.3.1.</span> <span class="toc-text">Memory</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Set"><span class="toc-number">1.0.3.2.</span> <span class="toc-text">Set</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Join"><span class="toc-number">1.0.3.3.</span> <span class="toc-text">Join</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Buffer"><span class="toc-number">1.0.3.4.</span> <span class="toc-text">Buffer</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.0.4.</span> <span class="toc-text">日志类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#TinyLog"><span class="toc-number">1.0.4.1.</span> <span class="toc-text">TinyLog</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#StripeLog"><span class="toc-number">1.0.4.2.</span> <span class="toc-text">StripeLog</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Log"><span class="toc-number">1.0.4.3.</span> <span class="toc-text">Log</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A5%E5%8F%A3%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.0.5.</span> <span class="toc-text">接口类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Merge"><span class="toc-number">1.0.5.1.</span> <span class="toc-text">Merge</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dictionary"><span class="toc-number">1.0.5.2.</span> <span class="toc-text">Dictionary</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Distributed"><span class="toc-number">1.0.5.3.</span> <span class="toc-text">Distributed</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.0.6.</span> <span class="toc-text">其它类型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Live-View"><span class="toc-number">1.0.6.1.</span> <span class="toc-text">Live View</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Null"><span class="toc-number">1.0.6.2.</span> <span class="toc-text">Null</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#URL"><span class="toc-number">1.0.6.3.</span> <span class="toc-text">URL</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/04/11/ClickHouse%20%E4%B8%AD%E5%85%B6%E5%AE%83%E5%B8%B8%E8%A7%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E(%E5%85%AB)/" title="ClickHouse 中其它常见的表引擎(八)">ClickHouse 中其它常见的表引擎(八)</a><time datetime="2023-04-11T09:04:31.000Z" title="发表于 2023-04-11 17:04:31">2023-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/04/11/ClickHouse%20%E4%B8%AD%E7%9A%84%E5%B8%B8%E7%94%A8%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0(%E5%8D%81%E4%B8%80)/" title="ClickHouse 中的常用聚合函数(十一)">ClickHouse 中的常用聚合函数(十一)</a><time datetime="2023-04-11T09:04:31.000Z" title="发表于 2023-04-11 17:04:31">2023-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/04/11/ClickHouse%20%E4%B8%AD%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E8%A1%A8%E5%BC%95%E6%93%8E%EF%BC%9AMergeTree%20%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90(%E5%85%AD)/" title="ClickHouse 中最重要的表引擎：MergeTree 的深度原理解析(六)">ClickHouse 中最重要的表引擎：MergeTree 的深度原理解析(六)</a><time datetime="2023-04-11T09:04:31.000Z" title="发表于 2023-04-11 17:04:31">2023-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/04/11/ClickHouse%20%E4%B9%8B%20MergeTree%20%E5%AE%B6%E6%97%8F%E4%B8%AD%E7%9A%84%E5%85%B6%E5%AE%83%E8%A1%A8%E5%BC%95%E6%93%8E(%E4%B8%83)/" title="ClickHouse 之 MergeTree 家族中的其它表引擎(七)">ClickHouse 之 MergeTree 家族中的其它表引擎(七)</a><time datetime="2023-04-11T09:04:31.000Z" title="发表于 2023-04-11 17:04:31">2023-04-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/04/11/ClickHouse%20%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E4%BB%A5%E5%8F%8A%E5%90%84%E7%A7%8D%E5%AD%90%E5%8F%A5(%E4%B9%9D)/" title="ClickHouse 中的数据查询以及各种子句(九)">ClickHouse 中的数据查询以及各种子句(九)</a><time datetime="2023-04-11T09:04:31.000Z" title="发表于 2023-04-11 17:04:31">2023-04-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By wenwen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>